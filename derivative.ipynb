{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of f(x) at x=2: 7.000010000091094\n",
      "Symbolic derivative of f(x): 2*x + 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sympy import symbols, diff\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 2\n",
    "\n",
    "# Compute the derivative of f(x) using the finite difference method\n",
    "def derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# Test\n",
    "x_value = 2\n",
    "print(f\"Derivative of f(x) at x={x_value}: {derivative(f, x_value)}\")\n",
    "# Define the symbolic variable and function\n",
    "x = symbols('x')\n",
    "f_sym = x**2 + 3*x + 2\n",
    "\n",
    "# Compute the derivative symbolically\n",
    "f_derivative = diff(f_sym, x)\n",
    "print(f\"Symbolic derivative of f(x): {f_derivative}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Derivative with respect to x at (x=1, y=2): 6.000009999951316\n",
      "Partial Derivative with respect to y at (x=1, y=2): 6.000009999951316\n",
      "Symbolic Partial Derivative with respect to x: 2*x + 2*y\n",
      "Symbolic Partial Derivative with respect to y: 2*x + 2*y\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "# Define the multivariate function f(x, y)\n",
    "def f(x, y):\n",
    "    return x**2 + 2*x*y + y**2\n",
    "\n",
    "# Compute partial derivative with respect to x\n",
    "def partial_derivative_x(f, x, y, h=1e-5):\n",
    "    return (f(x + h, y) - f(x, y)) / h\n",
    "\n",
    "# Compute partial derivative with respect to y\n",
    "def partial_derivative_y(f, x, y, h=1e-5):\n",
    "    return (f(x, y + h) - f(x, y)) / h\n",
    "\n",
    "# Test\n",
    "x_value, y_value = 1, 2\n",
    "print(f\"Partial Derivative with respect to x at (x={x_value}, y={y_value}): {partial_derivative_x(f, x_value, y_value)}\")\n",
    "print(f\"Partial Derivative with respect to y at (x={x_value}, y={y_value}): {partial_derivative_y(f, x_value, y_value)}\")\n",
    "# Define symbolic variables\n",
    "x, y = symbols('x y')\n",
    "\n",
    "# Define the symbolic multivariate function\n",
    "f_sym_multivariate = x**2 + 2*x*y + y**2\n",
    "\n",
    "# Compute symbolic partial derivatives\n",
    "partial_derivative_x_sym = diff(f_sym_multivariate, x)\n",
    "partial_derivative_y_sym = diff(f_sym_multivariate, y)\n",
    "\n",
    "print(f\"Symbolic Partial Derivative with respect to x: {partial_derivative_x_sym}\")\n",
    "print(f\"Symbolic Partial Derivative with respect to y: {partial_derivative_y_sym}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = 7.699999999999999, f(x) = 84.38999999999999\n",
      "Iteration 2: x = 5.859999999999999, f(x) = 53.91959999999999\n",
      "Iteration 3: x = 4.388, f(x) = 34.418544\n",
      "Iteration 4: x = 3.2104, f(x) = 21.93786816\n",
      "Iteration 5: x = 2.26832, f(x) = 13.950235622400001\n",
      "Iteration 6: x = 1.514656, f(x) = 8.838150798335999\n",
      "Iteration 7: x = 0.9117248, f(x) = 5.56641651093504\n",
      "Iteration 8: x = 0.42937983999999996, f(x) = 3.472506566998425\n",
      "Iteration 9: x = 0.043503871999999943, f(x) = 2.132404202878992\n",
      "Iteration 10: x = -0.2651969024000001, f(x) = 1.2747386898425548\n"
     ]
    }
   ],
   "source": [
    "# Define the function and its derivative (gradient)\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 2\n",
    "\n",
    "def df(x):\n",
    "    return 2*x + 3\n",
    "\n",
    "# Gradient Descent function\n",
    "def gradient_descent(starting_point, learning_rate, n_iterations):\n",
    "    x = starting_point\n",
    "    for i in range(n_iterations):\n",
    "        gradient = df(x)  # Compute the gradient\n",
    "        x = x - learning_rate * gradient  # Update x\n",
    "        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}\")\n",
    "\n",
    "# Test gradient descent\n",
    "starting_point = 10  # Initial guess\n",
    "learning_rate = 0.1\n",
    "n_iterations = 10\n",
    "gradient_descent(starting_point, learning_rate, n_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2978475033585172\n",
      "Epoch 1000, Loss: 0.25026376776848724\n",
      "Epoch 2000, Loss: 0.25020854481414606\n",
      "Epoch 3000, Loss: 0.2501620639691526\n",
      "Epoch 4000, Loss: 0.2501220230240266\n",
      "Epoch 5000, Loss: 0.2500866825367864\n",
      "Epoch 6000, Loss: 0.25005466733317533\n",
      "Epoch 7000, Loss: 0.25002485398418617\n",
      "Epoch 8000, Loss: 0.24999629222155978\n",
      "Epoch 9000, Loss: 0.24996814847903748\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the neural network\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Simple 2-layer neural network\n",
    "def forward_pass(X, weights1, weights2):\n",
    "    z1 = np.dot(X, weights1)\n",
    "    a1 = sigmoid(z1)  # Activation\n",
    "    z2 = np.dot(a1, weights2)\n",
    "    a2 = sigmoid(z2)\n",
    "    return a1, a2  # Return activations\n",
    "\n",
    "# Backpropagation to compute gradients\n",
    "def backpropagate(X, y, a1, a2, weights1, weights2):\n",
    "    m = X.shape[0]  # Number of samples\n",
    "    # Compute the error (loss gradient) for output layer\n",
    "    error_output = a2 - y\n",
    "    d_z2 = error_output * sigmoid_derivative(a2)\n",
    "\n",
    "    # Gradient of the weights between layer 1 and layer 2\n",
    "    grad_weights2 = np.dot(a1.T, d_z2) / m\n",
    "\n",
    "    # Compute the error for the hidden layer\n",
    "    d_a1 = np.dot(d_z2, weights2.T)\n",
    "    d_z1 = d_a1 * sigmoid_derivative(a1)\n",
    "\n",
    "    # Gradient of the weights between input and hidden layer\n",
    "    grad_weights1 = np.dot(X.T, d_z1) / m\n",
    "\n",
    "    return grad_weights1, grad_weights2\n",
    "\n",
    "# Training with a small dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input\n",
    "y = np.array([[0], [1], [1], [0]])  # Output (XOR function)\n",
    "\n",
    "# Random initialization of weights\n",
    "np.random.seed(42)\n",
    "weights1 = np.random.rand(2, 3)  # 2 input neurons, 3 hidden neurons\n",
    "weights2 = np.random.rand(3, 1)  # 3 hidden neurons, 1 output neuron\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.1\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    a1, a2 = forward_pass(X, weights1, weights2)\n",
    "\n",
    "    # Backpropagation\n",
    "    grad_weights1, grad_weights2 = backpropagate(X, y, a1, a2, weights1, weights2)\n",
    "\n",
    "    # Update weights\n",
    "    weights1 -= learning_rate * grad_weights1\n",
    "    weights2 -= learning_rate * grad_weights2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean((a2 - y)**2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
